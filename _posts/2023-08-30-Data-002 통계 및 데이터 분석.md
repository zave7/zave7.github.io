---
title: 통계 및 데이터 분석
categories: Data
---
통계 및 데이터 분석에 대해 학습한 전반적인 내용을 간략히 정리합니다.

### 통계

통계란 현상을 나타내고 있는 데이터로부터 유용한 정보를 도출하여 적절한 의사결정을 하게 해주는 것이다.

통계 분석의 알고리즘은 통계적 모델에 기반을 두고 설계되었으므로 기본적 통계 지식이 있어야 한다.

### 전수조사와 표본조사

- **전수조사**는 관심의 대상이 되는 집단을 이루는 모든 개체들들 조사하여 `모집단`의 특성을 측정하는 방법이다.
- **표본조사**는 관심의 대상이 되는 전체 모집단 중 일부를 선택하고 그 `선택된 일부`만을 대상으로 조사를 실시하여 이로부터 전체 모집단의 특정을 추정해 내는 것이다.
    - 표본조사는 전수조사에 비해 시간과 비용이 절감되고 심도있는 조사가 가능하다는 점에서 장점이 있다.
    - 다만, 모집단에서 추출한 소수의 표본이 전체 모집단의 특성을 잘 대표해야만 한다.
    - 즉, 표본조사는 소수의 표본을 통해 전체 모집단의 특성을 추정해 내는 것이므로 그 표본이 전체 모집단을 잘 대표할 때는 효과적으로 사용될 수 있지만 그렇지 않을 때는 문제가 생길 수도 있다.

### 알아두어야 할 통계 지식

- 가설 검정
    - 귀무 가설 : 기존의 사실과 차이가 없다. 새로울 게 없다는 가설이다.
    - 대립 가설 : 귀무 가설의 반대. 즉 검증하고 싶은 새로운 사실이다.
    - 귀무가설을 세우는 이유
        - 귀무가설은 기존에 알려져 있는 것이기 때문에 새로운 사실인 대립가설보다 더 정확하게 진술할 수 있기 때문이다. 더 정확하게 진술할 수 있다는 것은 상대적으로 더 쉽게 검증할 수 있다는 뜻이기도 하다.
- 유의 확률 (p-value)
    - 통계적 가설 검정에서 유의 확률(p-value)는 귀무가설이 맞다고 가정할 때 얻은 결과보다 극단적인 결과가 실제로 관측될 확률이다. 실험의 유의 확률은 실험 표본 공간에서 정의되는 확률변수로써, 0~1 사이의 값을 가진다.
    - p-value는 관찰된 데이터가 귀무가설과 양립하는 정도를 0에서 1 사이의 수치로 표현한 것이다. p-value가 작을수록 그 정도가 약하다고 본다.
    - 일반적으로 p-value가 0.05% 보다 작을 경우 귀무가설을 기각하고 대립가설을 채택할 수 있다. ( 모델 사용의 가능 여부를 판단하는 기준이 된다 )
- 1종 오류 vs 2종 오류
    - 1종 오류 : 귀무가설이 참인데 기각하는 경우
    - 2종 오류 : 귀무가설이 거짓인데 기각하지 않은 경우
    - < (예시) 무죄 추정의 원칙 >
        - 처음에 ‘범죄자는 범인이 아니다.’ 라는 귀무가설을 세우고 그 귀무가설을 기각할 수 있는 ‘범죄자는 범인이다’(대립가설)를 입증할 수 있는 자료를 모으는 것이다.
        - 범인이 아닌데 범인이라고 판단할 경우 대립 가설을 채택하는 경우에 오류(1종 오류)가 발생한다.
        - 잘못된 주장을 채택하는것을 방지하기 위함이다.
        - p-value는 1종 오류를 범할 확률이다.
- 요약
    1. 모집단, 전수 조사 (전수조사하는 일은 거의 없다)
    2. 표본, 표본 조사
    3. 유의수준 (오차 한계) : 5%
    4. 신뢰 수준 : 95%
    5. 가설 (귀무가설, 대립가설)
    6. p-value < 0.05, 대립 가설 채택
        1. 보편적으로 5%로 잡고, 분야에 따라 다를 수 있다.

*통계학의 메커니즘은 내가 주장하는 것을 대립가설에 대입하고 기존의 사실은 귀무가설에 넣는것이다.*

### 기술 통계

기술통계는 분설할 데이터의 전반적인 데이터 형태를 나타내는 것을 의미한다.

- 데이터의 중심을 의미하는 개념
    - 산술 평균 : 전체 변수들을 더하고 갯수로 나눈 값이다.
    - 중앙값 : 변수들의 크기를 순서대로 나열했을 때 가장 중앙에 있는 값이다.
        - 짝수개일 경우 가운데 두 수의 평균으로 계산한다.
        - 극단적인 예외 값이 있는 경우 유의미하게 활용될 수 있다.
    - 최빈값
        - 데이터에서 가장 많이 등장하는 값이다.
        - 똑같은 갯수의 값이 여러개일 경우 분야에 따라서 선택하거나 그 값들을 다 활용한다.
        - 극단적인 예외 값이 있는 경우 유의미하게 활용될 수 있다.
- 데이터의 흩어진 정도를 의미하는 개념
    - 편차 : 얼마나 값들이 떨어져있는지에 대한 값이다.
    - 편차의 제곱 : 음의 값들을 제거하기 위해 제곱
    - 분산 : 편자의 제곱을 나눈값 (평균) / 분포를 파악하기 위해 평균을 구한다
    - 표준 편차 : 분산에 루트를 씌운 값 (제곱근) / 값이 너무 커지는 것을 방지
        - 변동 계수 : 표준 편차를 표본 평균이나 모 평균 등 산술 평균으로 나눈 것이다.
- 데이터의 비대칭성을 의미하는 개념
    - 왜도 : 데이터가 대칭을 이룰 수록 왜도 값은 0에 수렴한다.
        - 양의 수는 오른쪽에 데이터가 더 많다는 것을 의미한다.
        - 데이터가 한쪽으로 치우칠 수록 양수 또는 음수로 나타난다.
- 데이터 분포의 꼬리부분의 길이와 중앙부분의 뾰족함을 의미하는 개념
    - 첨도 : 완전히 정규 분포를 따르는 데이터의 첨도값은 0이다.
        - 데이터의 꼬리의 모습에 따라 양수 또는 음수로 나타난다.
        - 0이 아닌 첨도값은 데이터가 정규 분포로부터 얼마나 벗어나있는지 알게 해준다.
- 추가 내용
    - 박스 플롯(box plot)
        - 박스 플롯은 데이터의 대략적인 분포와 개별적인 이상치들을 동시에 보여줄 수 있으며 서로 다른 데이터 뭉치를 쉽게 비교할 수 있도록 도와주는 시각화 기법이다.

기술 통계와 추리통계에 대한 내용 : [https://drhongdatanote.tistory.com/25](https://drhongdatanote.tistory.com/25)

### 상관 분석

상관 분석은 두 변수 간에 어떤 선형적 관계를 갖고 있는 지를 분석하는 방법이다.

- 상관관계
    - 두 변수는 서로 독립적인 관계이거나 상관된 관계일 수 있으며 이때 두 변수간의 관계의 강도를 상관관계라고 한다.
    - 상관관계의 정도를 파악하는 상관 계수는 두 변수간의 연관된 정도를 나타낼 뿐 `인과관계`를 설명하는 것은 아니다.
    - 두 변수간의 원인과 결과의 인과관계가 있는지에 대한 것은 `회귀분석`을 통해 인과관계의 방향, 정도와 수학적 모델을 확인해 볼 수 있다.
    - 다만, 상관관계를 바탕으로 인과관계를 더 들여다 볼 수 있다.
- 상관계수
    - 상관계수는 두 변수 사이의 통계적 관계를 표현하기 위해 특정한 상관관계의 정도를 수치적으로 나타낸 계수이다.
    - 여러 유형의 상관계수가 존재하지만 제각기 자신들만의 정의와 특징이 있다. 이들은 모두 값의 범위가 -1에서 +1사이에 속하며 여기서 ±1은 정도가 가장 센 잠재적 일치를 나타내고 0은 정도가 가장 센 불일치를 나타낸다. ( 그래프의 우상향과, 좌상향 )
- 인과 관계
    - 특징
        1. 공변성 : 두가지 변수가 ‘함께 움직이는’ 경향이 있다.
        2. 시간적 선후관계 : 시간적으로 어느 하나가 먼저 변화 했을 때 다른 하나가 뒤따라 변화하는 관계이다.
        3. 비허위성 : 공변성과 선후관계의 양상이 제 3의 다른 변인으로 설명될 수 없어야 한다.
        - 비허위성을 전제로 `공변성`과 `시간적 선후관계` 둘 중 하나라도 만족할 경우 인과 관계가 있다고 할 수 있다.
    - 종속 변수(결과 변수, y)
        - 다른 변수에 영향을 받지만 다른 변수에 영향을 미칠 수 없는 변수로써 인과관계에서 결과를 나타내는 변수이다.
    - 독립 변수(원인 변수, x)
        - 인과관계에서 다른 변수의 변화를 일으키는 변수로써 인과에서 원인을 나타내는 변수이다.

### 회귀분석

회귀분석은 독립변수가 종속변수에 영향을 미치는지 알아보고자 할 때 실시하는 분석방법이다.

- 회귀분석의 목적
    - 한 변수의 값으로부터 다른 변수의 값을 예측하고자 하는 목적을 갖고있다. 따라서, 독립 변수와 종속 변수 사이의 관계를 기술하고 설명할 수 있다.
    - 실험 설계에서는 연구자가 연구내용, 연구목적, 연구가설을 고려해서 수집된 자료로부터 어떤 분석방법이 적합할 것인지 예측할 수 있다.
- 잔차
    - 잔차는 표본집단에서 회귀식을 통해 얻은 예측값과 관측값의 차이이다. ( 오차는 모집단에서 회귀식을 통해 얻은 예측값과 관측값의 차이, ε )
    - 잔차를 모두 더했을 때 0에 수렴해야한다.
    - 사실상 현상을 분석할때, 모집단의 모든 데이터를 축적하기 보다, 일부의 데이터(표본집단)에서 회귀식을 얻기 때문에, 잔차를 기준으로 회귀식의 최적 회귀계수를 추정한다.
- 결정 계수(R²,R Square) 0~1
    - 결정계수는 회귀 모델에서 독립변수가 종속변수를 얼마만큼 설명해 주는지를 가키리는 지표이다. ( y의 변화량이 x의 변화량에 따라서 설명될 수 있는 정도 )
    - 결정 계수가 높을수록 독립변수가 종속변수를 많이 설명할 수 있다는 의미이다.
    - 다만, 독립변수의 개수가 증가하면 일반적으로 증가하는 현상이 있다. 따라서, 대안으로 조정된 결정계수를 같이 계산한다.

### 다중 회귀분석

다중회귀분석은 다양한 독립변수가 종속변수에 얼마나 영향을 주는지에 대한 분석 방법이다.

- 다중회귀분석에서 예측변수 갯수가 많을 경우, 적절한 회귀모형 선택이 필요하다.
    - 회귀모형에 포함되는 예측변수의 선정 기순
        - 종속변수와 높은 상관관계
        - 선택된 예측변수들은 서로 낮은 상관관계를 보임
        - 예측 변수의 갯수는 적을수록 유리(차원의 저주)
- 변수 선택법(**Feature selection)**
    - All possible regressions
        - 변수들의 가능한 모든 조합들로부터 최적의 모형을 찾아낸다.
        - 유의한 변수가 누락되지 않는 안전한 방법이다.
        - 변수가 많을수록 탐색 시간이 급증한다.
    - Forward stepwise selection (Forward selection)
        - 기여도가 높은 유의한 변수부터 하나씩 추가하는 기법이다.
        - 빠른 계산이 장점이다.
        - 이미 선택된 변수는 다시 제거되지 않는다.
    - Backward stepwise selection (Backward elimination)
        - 모든 변수를 포함한 상태에서 불필요한 변수를 제거해나가는 방법이다.
        - 중요한 변수가 제외될 가능성이 매우 적다.
        - 이미 제외된 변수는 다시 선택되지 않는다.